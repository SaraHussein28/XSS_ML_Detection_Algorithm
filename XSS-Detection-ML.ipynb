{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XSSClassifier:  Detection of XSS Attacks Using Machine Learning Approach\n",
    "\n",
    "### Research Paper:\n",
    "\n",
    "Based on this paper, machine learning classifiers are used for categorizing webpages into either XSS or non-XSS. The process primarily encompasses four steps: identifying features, gathering webpages, extracting features, and constructing a training dataset, and employing machine learning classification. \n",
    "\n",
    "https://pdfs.semanticscholar.org/2c74/d8e94b73c35e8651189262d0c7f32e6cfc7c.pdf?_ga=2.60428845.1871262773.1581209097-1873828646.1581209097\n",
    " \n",
    " \n",
    " ### Goal:\n",
    "    Create a training algorithm for detecting an XSS attack vector in a Query Parameter string. \n",
    "    Define a set of features that we can use to train the model and label a sample as XSS(1) or Not XSS(0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gather Test Data \n",
    "Get a list of HTTP requests samples generated from different webpages from mentiond sources in the paper: XSSed, Alexa & Elgg. and create an array with labels non_xss_count = (0), xss_count = (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.25.2)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.26.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (115 kB)\n",
      "Using cached numpy-1.26.2-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[33mDEPRECATION: jupyter-server 2.0.0 has a non-standard dependency specifier jupyter-core!=~5.0,>=4.12. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of jupyter-server or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scipy 1.9.3 requires numpy<1.26.0,>=1.18.5, but you have numpy 1.26.2 which is incompatible.\n",
      "moviepy 1.0.3 requires imageio<3.0,>=2.5; python_version >= \"3.4\", but you have imageio 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.2\n",
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (1.9.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (6.4.0)\n",
      "Collecting numpy>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.25.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Using cached numpy-1.25.2-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[33mDEPRECATION: jupyter-server 2.0.0 has a non-standard dependency specifier jupyter-core!=~5.0,>=4.12. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of jupyter-server or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.2\n",
      "    Uninstalling numpy-1.26.2:\n",
      "      Successfully uninstalled numpy-1.26.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "moviepy 1.0.3 requires imageio<3.0,>=2.5; python_version >= \"3.4\", but you have imageio 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.25.2\n",
      "Requirement already satisfied: python-Levenshtein in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.23.0)\n",
      "Requirement already satisfied: Levenshtein==0.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-Levenshtein) (0.23.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Levenshtein==0.23.0->python-Levenshtein) (3.5.2)\n",
      "\u001b[33mDEPRECATION: jupyter-server 2.0.0 has a non-standard dependency specifier jupyter-core!=~5.0,>=4.12. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of jupyter-server or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (4.64.1)\n",
      "\u001b[33mDEPRECATION: jupyter-server 2.0.0 has a non-standard dependency specifier jupyter-core!=~5.0,>=4.12. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of jupyter-server or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "\u001b[33mDEPRECATION: jupyter-server 2.0.0 has a non-standard dependency specifier jupyter-core!=~5.0,>=4.12. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of jupyter-server or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mGathering Data...\n",
      "*"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sarahussein/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -U numpy\n",
    "!{sys.executable} -m pip install -U gensim\n",
    "!{sys.executable} -m pip install -U python-Levenshtein\n",
    "!{sys.executable} -m pip install -U nltk\n",
    "!{sys.executable} -m pip install -U scikit-learn\n",
    "import warnings\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from numpy import *\n",
    "from urllib.parse import unquote\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import urllib.parse as parse\n",
    "import pickle\n",
    "\n",
    "\n",
    "xss_count = 0\n",
    "non_xss_count = 0\n",
    "\n",
    "test_XSS_strings = []\n",
    "test_normal_string = []\n",
    "\n",
    "temp_x = []\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "print(\"Gathering Data...\")\n",
    "# gather the XSS string and append the label of 1 to y array\n",
    "with open('lib/testXSS.txt', 'r') as f:\n",
    "    test_XSS_strings = f.readlines()\n",
    "print(\"*\", sep=' ', end='', flush=True)\n",
    "# parse out the query part of the URL \n",
    "for line in test_XSS_strings:\n",
    "    query = parse.urlsplit(line)[3]\n",
    "    #try to remove open redirect vulns\n",
    "    if \"?http\" in str(line):\n",
    "        continue\n",
    "    if \"?url=http\" in str(line):\n",
    "        continue\n",
    "    if \"?fwd=http\" in str(line):\n",
    "        continue\n",
    "    if \"?path=http\" in str(line):\n",
    "        continue\n",
    "    if \"=http\" in str(query):\n",
    "        continue\n",
    "    if \"page=search\" in str(query):\n",
    "        continue\n",
    "    if len(query) > 8:\n",
    "        xss_count += 1\n",
    "        temp_x.append(line)\n",
    "        \n",
    "# remove duplicates\n",
    "dedup = list(dict.fromkeys(temp_x))\n",
    "print(\"*\", sep=' ', end='', flush=True)\n",
    "# Add a feature to X and label to the y array\n",
    "for line in dedup:\n",
    "    X.append(line)\n",
    "    y.append(1)\n",
    "    \n",
    "temp_x = []\n",
    "dedup = []\n",
    "print(\"*\", sep=' ', end='', flush=True)\n",
    "\n",
    "# gather the list of normal string and append the label of 0 to y array \n",
    "with open('lib/testNORM.txt', 'r') as f:\n",
    "    test_normal_string = f.readlines()\n",
    "    \n",
    "# parse out the query part of the URL \n",
    "for line in test_normal_string:\n",
    "    query = parse.urlsplit(line)[3]\n",
    "    if len(query) > 3:\n",
    "        non_xss_count += 1\n",
    "        temp_x.append(line)\n",
    "        \n",
    "# remove duplicates\n",
    "dedup = list(dict.fromkeys(temp_x))\n",
    "print(\"*\", sep=' ', end='', flush=True)\n",
    "# Add a feature to X and a label to the y array\n",
    "for line in dedup:\n",
    "    X.append(line)\n",
    "    y.append(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of XSS Samples: 38608\n",
      "Number of NOT XSS Samples: 44826\n",
      "Total Samples: 83434\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of XSS Samples: \"+str(xss_count))\n",
    "print(\"Number of NOT XSS Samples: \"+str(non_xss_count))\n",
    "print(\"Total Samples: \"+str(xss_count+non_xss_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The sample set collected was larger then the one used in the reasearch paper\n",
    "```\n",
    "The paper has a training dataset containing 1,000 webpages (400 malicious and 600 benign) and their extracted features.\n",
    "\n",
    "```\n",
    "We expanded the data set and samples used to train the model with bengin and malicious samples from the attached resources:\n",
    "\n",
    "Benign:\n",
    "\n",
    "https://raw.githubusercontent.com/Xyntax/ML/master/DL_for_xss/data/normal_examples.csv\n",
    "https://raw.githubusercontent.com/Xyntax/ML/master/DL_for_xss/data/white.csv\n",
    "\n",
    "Malicious:\n",
    "\n",
    "https://gist.github.com/ThomasOrlita/e2e4a6d72877c8c897082eefe969578a\n",
    "https://raw.githubusercontent.com/Xyntax/ML/master/DL_for_xss/data/xssed.csv\n",
    "https://raw.githubusercontent.com/Xyntax/ML/master/DL_for_xss/data/black.csv\n",
    "https://raw.githubusercontent.com/foospidy/payloads/master/other/xss/reddit_xss_get.txt\n",
    "https://github.com/danielmiessler/SecLists/tree/master/Fuzzing/XSS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an array of the features for each test sample\n",
    "\n",
    "Certain HTML tags can be used by an attacker to inject the XSS code scripts from outside. These HTML tags consist of ```<link>, <object>, <form>, <script>, <embed>, <ilayer>, <layer>, <style>, <applet>, <meta>, <img>, <iframe>```. \n",
    "\n",
    "Also, attacker can misuse some methods on the embedded XSS payload such as ```exec(), fromCharCode(), eval, alert(), getElementsByTagName(), write(), unscape(), and escape()```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to convert an array of query strings to a set of features\n",
    "def getVec(text):\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(text)]\n",
    "    max_epochs = 25\n",
    "    vec_size = 20\n",
    "    alpha = 0.025\n",
    "\n",
    "    model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm=1)\n",
    "    model.build_vocab(tagged_data)\n",
    "    print(\"Building the sample vector model...\")\n",
    "    features = []\n",
    "    for epoch in range(max_epochs):\n",
    "        print(\"*\", sep=' ', end='', flush=True)\n",
    "        model.random.seed(42)\n",
    "        model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "        # decrease the learning rate\n",
    "        model.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "    model.save(\"lib/d2v.model\")\n",
    "    print()\n",
    "    print(\"Model Saved\")\n",
    "    for i, line in enumerate(text):\n",
    "        featureVec = [model.dv[i]]\n",
    "        lineDecode = unquote(line)\n",
    "        lineDecode = lineDecode.replace(\" \", \"\")\n",
    "        lowerStr = str(lineDecode).lower()\n",
    "  \n",
    "        # Features related to malicious HTML tags\n",
    "        malicious_tags = ['<link', '<object', '<form', '<embed', '<ilayer', '<layer', '<style', '<applet', '<meta', '<img',\n",
    "                           '<iframe', '<input', '<body', '<video', '<button', '<math', '<picture', '<map', '<svg', '<div',\n",
    "                           '<a', '<details', '<frameset', '<table', '<comment', '<base', '<image']\n",
    "        feature1 = 0\n",
    "        for tag in malicious_tags:\n",
    "            feature1+= int(lowerStr.count(tag))\n",
    "\n",
    "        # Features related to malicious methods/events\n",
    "        malicious_methods = ['exec', 'fromcharcode', 'eval', 'alert', 'getelementsbytagname', 'write', 'unescape',\n",
    "                             'escape', 'prompt', 'onload', 'onclick', 'onerror', 'onpage', 'confirm', 'marquee']\n",
    "        feature2 = 0\n",
    "        for method in malicious_methods:\n",
    "            feature2+= int(lowerStr.count(method))\n",
    "            \n",
    "        # Features related to file extensions and keywords\n",
    "        feature3 = int(lowerStr.count('.js'))\n",
    "        feature4 = int(lowerStr.count('javascript'))\n",
    "\n",
    "        # Other features\n",
    "        feature5 = int(len(lowerStr))\n",
    "        feature6 = int(lowerStr.count('<script')) + int(lowerStr.count('&lt;script')) + int(lowerStr.count('%3cscript')) + int(lowerStr.count('%3c%73%63%72%69%70%74'))\n",
    "       \n",
    "        feature7 = 0\n",
    "        for char in '&<>\"\\'/%*;+=%3C':\n",
    "            feature7 +=int(lowerStr.count(char))\n",
    "        feature8 = int(lowerStr.count('http'))\n",
    "\n",
    "          # append the features\n",
    "        featureVec = np.append(featureVec,feature1)\n",
    "        #featureVec = np.append(featureVec,feature2)\n",
    "        featureVec = np.append(featureVec,feature3)\n",
    "        featureVec = np.append(featureVec,feature4)\n",
    "        featureVec = np.append(featureVec,feature5)\n",
    "        featureVec = np.append(featureVec,feature6)\n",
    "        featureVec = np.append(featureVec,feature7)\n",
    "        \n",
    "#         feature_vec = [model.dv[text.index(line)], feature1, feature2, feature3, feature4, feature5, feature6, feature7, feature8]\n",
    "        \n",
    "        features.append(featureVec)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the sample vector model...\n",
      "*************************\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "features = getVec(X)\n",
    "features_dict = {'data':X,'features':features,'label':y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sample: http://search.rin.ru/cgi-bin/find.cgi?text=%3Cscript%3Ealert(%27HZ+iz+1337%27)%3B%3C%2Fscript%3E\n",
      "\n",
      "Features: [ 4.40686792e-01 -2.94887638e+00 -1.04484606e+00  1.64985323e+00\n",
      " -6.61160231e-01 -1.50925446e+00 -1.78789949e+00 -1.46867549e+00\n",
      "  1.67239487e+00 -3.88574392e-01  1.80456448e+00  3.24892545e+00\n",
      "  6.38268948e-01 -4.36293446e-02 -1.00890172e+00  2.35312438e+00\n",
      " -1.18228400e+00  3.47410703e+00  3.25613528e-01 -7.81922162e-01\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  8.10000000e+01\n",
      "  1.00000000e+00  1.70000000e+01]\n",
      "\n",
      "Label:\u001b[1;31;1m XSS(1)/\u001b[1;32;1m NOT XSS(0)\u001b[0;0m: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Sample: \"+ X[0])\n",
    "print(\"Features: \" + str(features[0]))\n",
    "print(\"\\nLabel:\\033[1;31;1m XSS(1)/\\033[1;32;1m NOT XSS(0)\\033[0;0m: \" + str(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Model\n",
    "\n",
    "Split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, y, test_size = .3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(random_state=42)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use RandomState for reproducibility.\n",
    "from sklearn import tree\n",
    "my_classifier1 = tree.DecisionTreeClassifier(random_state=42)\n",
    "print(my_classifier1)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additonal tunning of hyperparameters can be done.\n",
    "\n",
    "I have only set the KNN n_neighbors weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier #1 DecisionTreeClassifier\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Classifier #1 DecisionTreeClassifier\")\n",
    "my_classifier1.fit(X_train, y_train)\n",
    "predictions1 = my_classifier1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation Metrics\n",
    "\n",
    "\n",
    "### Predictions Results\n",
    "\n",
    "Test the classifier and obtain its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score #1: 98.4%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy Score #1: {:.1%}'.format(accuracy_score(y_test, predictions1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How did the test compare with the research paper\n",
    "\n",
    "Comparing the data with the results seen in the research paper we are within the expected accuracy range.\n",
    "```\n",
    "Every classifier had accuracy of more than 0.947.\n",
    "\n",
    "These results validate the effectiveness of our proposed approach. The overall result of all classifiers shows that \n",
    "RandomForest and MLP are the best classifiers in terms of acuuracy 0.995\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
